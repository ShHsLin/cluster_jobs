#!/bin/bash

#SBATCH --chdir ./                      # output files realtive to the directory from where we submit jobs
#SBATCH --job-name {jobname!s}          # jobname
#SBATCH --output ./%x.%N.%j.out         # this is where the output goes. %x=Job name, %j=Jobd id, %N=node.
#SBATCH --get-user-env                  # this is needed
#SBATCH --clusters=kcs_nim              # selects the nim cluster
#SBATCH --partition=kcs_nim_batch       # ...
#SBATCH --reservation=kcs_nim_users     # ...
#SBATCH --export=NONE                   # this is needed
#SBATCH --nodes=1-1                     # select a full node (64 cores). 
#                               !!! NOTE: On the NIM-cluster, you can only choose FULL Nodes, so please submit 
#                                         enough jobs in a shared memory system (e.g. parameter sweep) to really 
#                                         make use of all 64 cores and not block them unnecessarily!!!
#SBATCH --cpus-per-task=64   # we can only reserve full nodes with 64 cores/threads
#SBATCH --mail-type={mailtype!s}        # you will receive a mail should your job fail. You can also choose NONE in case you don't want to be emailed
#SBATCH --mail-user={email!s}           # you have to enter an email address here
#SBATCH --time={cpu!s}                  # enter a maximum runtime for the job. (format: days-hours:minutes:seconds)
#                                         Maximum runtime on nim cluster: 3-00:00:00  (``scontrol show partition --clusters=kcs_nim``)
{more_options!s}

source /etc/profile.d/modules.sh        # load the modules system of the LRZ

module load python/2.7_intel            # load the module intelpython.
# you can check out all available modules (e.g. python/2.7_intel if you use python2)
# with the command `module avail` in the command line

export MKL_NUM_THREADS=64               # number of cores per node, total for all the tasks below!
export MKL_DYNAMIC=FALSE                # important: use hyperthreading and not just the number of physical cores.

# if needed, you can set PYTHONPATH here to include other libraries, e.g.
TENPYPATH="/dss/dssfs02/lwp-dss-0001/pn69yo/pn69yo-dss-0000/shared/prev_tenpy_20191009_master_for_python_2.7_intel"
test -d "$TENPYPATH" || ( echo "can't find $TENPYPATH" && exit 1)
export PYTHONPATH="$TENPYPATH"

echo "Execute  job on host $HOSTNAME at $(date)"

# some bash functionality allowing to distribute tasks to different nodes equally
# with a single script, just changing the {job_id} parameter (from 1 to the number of nodes)
TASK_STARTS=(0 {task_starts!s})
TASK_STOPS=(0 {task_stops!s})
TASK_START=${{TASK_STARTS[{job_id!s}]}}
TASK_STOP=${{TASK_STOPS[{job_id!s}]}}
for (( TASKID=$TASK_START ; TASKID <= $TASK_STOP ; TASKID++ ))
do
    OUTPUTFILE="$SLURM_JOB_NAME.$TASKID.$SLURM_JOB_ID.out"              # separate output file for each task
    python {sim_file!s} {config_file!s} $TASKID &> $OUTPUTFILE &  # the actual jobs; $TASKID is an input parameters;
    echo "started task $TASKID, writing to $OUTPUTFILE"
done
wait  # ensures that the job is not terminated unless all tasks have been completed, except for exceeding the runtime
echo "finished job at $(date)"
